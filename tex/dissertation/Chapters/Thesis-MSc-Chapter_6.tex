% #############################################################################
% This is Chapter 6
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Conclusion and Future Work}
\cleardoublepage
% The following line allows to ref this chapter
\label{chap:conclusion}

This thesis presents Aerial\mbox{-}D together with an end-to-end methodology that transforms existing aerial segmentation datasets into a large-scale repository of referring expressions. The approach begins with a rule-driven generator that systematically converts masks into natural language descriptions grounded on spatial location, visual appearance, and relational cues. This foundation is then enhanced through a Large Language Model rewriting step supported by a cost-efficient distillation procedure that transfers high-quality capabilities from OpenAI's o3 model into a locally deployable Gemma3 annotator~\cite{gemma3,o3}, ensuring that the methodology remains both effective and computationally affordable at scale.

To close the thesis, we first synthesize the main findings and contributions, and then outline forward-looking extensions and directions that build on this work.

% #############################################################################
\section{Conclusions}

The resulting dataset enables RSRefSeg to be trained jointly across five datasets—Aerial-D, RRSIS-D, NWPU-Refer, RefSegRS, and Urban1960SatSeg—establishing reproducible baselines on Aerial\mbox{-}D while remaining competitive with published results on the external benchmarks~\cite{yuan2023rrsis,liu2024rotated,yang2024large,hao2025urban1960satseg}. These results validate both the quality of the automatically generated annotations and the capacity of the RSRefSeg architecture to leverage them alongside manually curated resources.

By pairing these evaluations with ablations on expression sources and historic-image filters, this work demonstrates that Aerial\mbox{-}D delivers a harder dataset for referring segmentation in aerial photographs and highlights the specific ingredients—linguistically varied expressions and synthetic degradation augmentation—that improve robustness across contemporary and archival imagery.

The key contributions can be summarized as follows: a comprehensive toolchain for generating referring expressions from instance/semantic annotations, the construction of Aerial-D as a large-scale automatically generated dataset, and a unified model that integrates historic-image transformations across the training pipeline to deliver reliable segmentation under degraded archival conditions.

% #############################################################################
\section{Future Work}

Future work can extend this foundation in three promising directions that build upon the established pipeline and toolchain developed in this thesis.

\textbf{Expression Enhancement for Existing Datasets.} The LLM-based expression enhancement methodology developed for Aerial-D can be directly applied to enrich existing referring expression segmentation datasets. The native captions supplied with public benchmarks such as RRSIS-D and NWPU-Refer could be enhanced with the same visual grounding cues that proved effective in our work. The dual-task prompting strategy—generating both linguistic variations and visually detailed descriptions—would diversify the referring expressions in these datasets while maintaining their original annotation structure. This approach would create a unified, higher-variety training pool that combines the strengths of manual annotation quality with automatic expression diversification, potentially improving model generalization across datasets. The established distillation pipeline using o3 for initial high-quality generation followed by Gemma3 fine-tuning ensures this enhancement remains computationally feasible at scale.

\textbf{Multilingual Dataset Expansion.} Multilingual variants of the generated referring expressions can be produced while preserving full automation, enabling referring expression segmentation research across diverse linguistic contexts. Two complementary approaches can achieve this goal. First, the proven distillation recipe can be adapted for translation: prompting o3 to generate high-quality translations of English expressions into target languages, then training a Gemma3 student model to replicate these translations at scale across the full dataset. This approach preserves the domain-specific phrasing and technical accuracy characteristic of aerial imagery descriptions. Second, dedicated translation models such as Tower Instruct~\cite{towerinstruct} offer specialized capabilities for multilingual generation that could be integrated directly into the pipeline. More generally, any specialized translation model that maintains semantic fidelity while adapting to the aerial domain vocabulary could be employed, ensuring the multilingual corpus retains the rich descriptive qualities of the original English expressions. The resulting multilingual dataset would enable cross-lingual evaluation of referring expression segmentation models and support deployment in diverse international contexts.

\textbf{Expanding Beyond 21 Classes with Generative Segmentation.} A fundamental limitation of Aerial-D is its restriction to the 21 object and land-cover classes inherited from the source datasets iSAID and LoveDA. While the pipeline successfully generates unrestricted natural language expressions from these base annotations, the variety of segmentation targets themselves remains constrained by the original dataset taxonomies. To address this limitation and enable truly open-vocabulary aerial referring expression segmentation, future work can leverage emerging foundation models with native segmentation capabilities. Gemini 2.5~\cite{gemini25}, for example, can directly output complete segmentation masks and bounding boxes with remarkable generalization capabilities inherent to foundation-scale multimodal models. This capability could be employed to process every image in Aerial-D and generate additional instance segmentations beyond the original 21 classes, discovering and segmenting novel object categories not present in the source annotations.

The integration would follow a generation-and-filtering workflow similar in spirit to the established pipeline. Each aerial image would be processed through Gemini 2.5 to extract all visible objects and their corresponding masks, regardless of whether they belong to the predefined taxonomy. These machine-generated segmentations would then pass through a filtering stage to remove noise, low-confidence predictions, and redundant overlapping masks that might result from the generative process. The filtered masks could subsequently be processed through the rule-based expression generation pipeline to produce referring expressions, followed by LLM enhancement using the distilled Gemma3 model to ensure linguistic quality and visual detail. This approach would transform Aerial-D from a dataset limited to predefined categories into a genuinely open-vocabulary resource where both the language and the segmentation targets are unrestricted, potentially unlocking new capabilities for fine-grained aerial scene understanding across arbitrary object classes.