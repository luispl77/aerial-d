% #############################################################################
% This is Chapter 5
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter in the following line
\fancychapter{Experiments}
\cleardoublepage
% The following line allows to ref this chapter
\label{chap:evaluation}

This chapter revisits the experimental evaluation of AerialSeg with the fully updated results that were first consolidated for the article. The focus is on how the Aerial-D dataset and RSRefSeg backbone behave under multi-dataset training, expression-level ablations, and large language model selection. Every figure, table, and narrative now matches the article while the discussion expands the reasoning for the dissertation.

% #############################################################################
\section{Model Architecture}

In order to evaluate language-guided segmentation in aerial imagery, we require a model that can interpret free-form expressions while preserving pixel-level precision. Generic segmentation systems usually break down under the remote-sensing domain shift: objects are small, visually repetitive, and densely packed, making it difficult for architectures that were tuned for ground-level imagery. We therefore adopt RSRefSeg as our backbone because it couples a strong multimodal encoder with a high-capacity mask decoder, allowing the model to ground language descriptions even when training data is scarce.

Concretely, we implement RSRefSeg in PyTorch following the design shown earlier in Figure~\ref{fig:rsrefseg_architecture}. SigLIP2 supplies the joint language--image encoder, SAM provides the segmentation decoder, and LoRA adapters are inserted into the query and value projections of both backbones as well as the text encoder projections~\cite{siglip2,sam,lora,chen2025rsrefseg}. This configuration mirrors the original RSRefSeg recipe but is fine-tuned for aerial imagery, giving us a proven starting point that already performed well on RRSIS-D and eliminating uncertainty around architectural choices.

% #############################################################################
\section{Experimental Setup}

To produce reliable comparisons across datasets we standardize the training recipe around a moderate batch size and carefully tuned optimization settings. Large batches are impractical on the available hardware, so we use mini-batches of four images and accumulate gradients over two steps to emulate an effective batch of eight. Training runs with mixed precision, the AdamW optimizer~\cite{adamw} with an initial learning rate of $1\times 10^{-4}$, weight decay of $0.01$, polynomial decay with power $0.9$, and gradient clipping at $1.0$ to keep updates stable. The checkpoint combines the \texttt{SigLIP2-SO400M} encoder with \texttt{SAM-ViT-Large}~\cite{siglip2,sam}, ensuring that the language and vision components remain aligned despite their different native resolutions; images are therefore resized to $384\times384$ for SigLIP2 and $1024\times1024$ for SAM.

The experiments also need a balanced training mix that prevents Aerial-D from overwhelming other corpora while still exposing the model to the dataset's richer expressions. We therefore train a single combined model on Aerial-D (restricted to the Unique Expressions Only split), RRSIS-D, NWPU-Refer, RefSegRS, and Urban1960SatSeg~\cite{yuan2023rrsis,yang2024large,hao2025urban1960satseg}. For the four datasets that originate from contemporary imagery we inject the historic-filter augmentations described in Section~\ref{subsec:historic_filters}: twenty percent of training images are randomly replaced with a grayscale, sepia, or film-grain variant so the model repeatedly encounters archival degradations during learning. Evaluation uses the native validation split for each dataset, plus historic-filtered copies that transform one hundred percent of the validation images; these appear in the tables under the ``Hist.'' columns.

% #############################################################################
\section{Cross-Dataset Evaluation}

A central question is whether a single combined model can generalize across diverse aerial benchmarks while withstanding historic degradations. Table~\ref{tab:combined_training_results} reports validation metrics for the combined training run described above, including IoU@0.5/0.7/0.9 (equivalent to the Pass@ thresholds), mean IoU, and overall IoU.

Across datasets the model maintains strong performance despite the differing expression styles. On RRSIS-D it reaches 69.48\%/53.62\%/21.78\% at the three thresholds, with 61.70\% mIoU and 73.44\% oIoU; the historic variant remains close at \textcolor{blue}{58.35\%} mIoU and \textcolor{blue}{71.73\%} oIoU. NWPU-Refer records 40.53\%/27.65\%/8.45\% along with 37.89\% mIoU and 51.34\% oIoU, dropping only to \textcolor{blue}{31.52\%} mIoU and \textcolor{blue}{46.17\%} oIoU under filters. RefSegRS shows 41.30\%/9.05\%/2.09\%, 40.10\% mIoU, and 45.05\% oIoU, with historic performance at \textcolor{blue}{32.79\%} mIoU and \textcolor{blue}{36.74\%} oIoU. Urban1960SatSeg, already historic, posts 78.05\%/59.96\%/28.25\% together with 69.35\% mIoU and 87.27\% oIoU. Finally, Aerial-D itself delivers 60.10\%/43.95\%/13.05\% with 49.78\% mIoU and 63.44\% oIoU, establishing a baseline for future work on the dataset.

The model therefore matches or exceeds previously reported baselines on every public benchmark while simultaneously defining reference numbers for Aerial-D. The small gap between original and historic evaluations shows that the augmentation strategy meaningfully improves robustness without sacrificing accuracy on clean imagery.

\begin{table}[H]
\centering
\caption{Combined training performance across five aerial referring segmentation datasets (historic validation results in \textcolor{blue}{blue}).}
\label{tab:combined_training_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{IoU@0.5} & \textbf{IoU@0.7} & \textbf{IoU@0.9} & \multicolumn{2}{c}{\textbf{mIoU}} & \multicolumn{2}{c}{\textbf{oIoU}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & & & \textbf{Orig.} & \textbf{Hist.} & \textbf{Orig.} & \textbf{Hist.} \\
\midrule
Aerial-D & 60.10\% & 43.95\% & 13.05\% & 49.78\% & \textcolor{blue}{46.82\%} & 63.44\% & \textcolor{blue}{61.12\%} \\
RRSIS-D & 69.48\% & 53.62\% & 21.78\% & 61.70\% & \textcolor{blue}{58.35\%} & 73.44\% & \textcolor{blue}{71.73\%} \\
NWPU-Refer & 40.53\% & 27.65\% & 8.45\% & 37.89\% & \textcolor{blue}{31.52\%} & 51.34\% & \textcolor{blue}{46.17\%} \\
RefSegRS & 41.30\% & 9.05\% & 2.09\% & 40.10\% & \textcolor{blue}{32.79\%} & 45.05\% & \textcolor{blue}{36.74\%} \\
Urban1960SatSeg & 78.05\% & 59.96\% & 28.25\% & 69.35\% & N/A & 87.27\% & N/A \\
\bottomrule
\end{tabular}%
}
\end{table}

% #############################################################################
\section{Ablation Studies}

\subsection{Expression Enhancement Ablation}

In order to isolate how each expression type contributes to performance we rerun training on controlled subsets of Aerial-D. The combined model mixes rule-based sentences with two LLM-enhanced variants, which makes it difficult to attribute gains to any single source. We therefore train four lighter RSRefSeg models—rule-based only, enhanced only, unique expressions only, and a combined run—while switching to \texttt{SAM-ViT-Base} and a LoRA rank of $r=16$ so that the ablation can be executed quickly.

Each checkpoint is evaluated on the Aerial-D validation split and on three external datasets without additional fine-tuning. Table~\ref{tab:ablation_expression_types} lists the number of samples processed, the number of epochs completed, and the resulting mIoU/oIoU scores. The combined configuration unsurprisingly wins on Aerial-D with \textbf{49.33\%} mIoU and \textbf{64.30\%} oIoU, mirroring the validation distribution. However, the specialized subsets generalize best elsewhere: the language-focused Enhanced Only model reaches \textbf{41.63\%} mIoU and \textbf{42.48\%} oIoU on RRSIS-D, while the visually grounded Unique Expressions Only split delivers \textbf{24.68\%} mIoU and \textbf{29.22\%} oIoU on NWPU-Refer. RefSegRS benefits from combining language and visual cues, with the combined run nudging ahead of the others.

We monitor validation loss for each run and stop as soon as it rebounds, leading to two epochs for the much larger combined subset and four epochs for the smaller splits. Although the specialized models see fewer total samples, they often outperform the full mixture on external benchmarks, confirming that targeted expression diversity is more valuable than sheer volume. This result directly motivates using the unique-only slice when mixing Aerial-D with other datasets in the combined training experiments.

\begin{table}[H]
\centering
\caption{Expression enhancement ablation across four datasets.}
\label{tab:ablation_expression_types}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcc|cc|cc|cc|cc@{}}
\toprule
\multirow{2}{*}{\textbf{Training Configuration}} & \multirow{2}{*}{\textbf{Samples}} & \multirow{2}{*}{\textbf{Epochs}} & \multicolumn{2}{c|}{\textbf{Aerial-D}} & \multicolumn{2}{c|}{\textbf{RefSegRS}} & \multicolumn{2}{c|}{\textbf{RRSIS-D}} & \multicolumn{2}{c}{\textbf{NWPU-Refer}} \\
\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
 & & & \textbf{mIoU} & \textbf{oIoU} & \textbf{mIoU} & \textbf{oIoU} & \textbf{mIoU} & \textbf{oIoU} & \textbf{mIoU} & \textbf{oIoU} \\
\midrule
Rule-based Only & 371K & 4 & 34.57\% & 39.31\% & 3.73\% & 0.55\% & 34.22\% & 36.46\% & 16.78\% & 13.70\% \\
Enhanced Only & 364K & 4 & 46.45\% & 56.99\% & 5.75\% & 4.99\% & \textbf{41.63\%} & \textbf{42.48\%} & 21.89\% & 16.68\% \\
Unique Expressions Only & 382K & 4 & 46.54\% & 63.02\% & 18.32\% & 8.37\% & 31.78\% & 33.73\% & \textbf{24.68\%} & \textbf{29.22\%} \\
Combined All & 1{,}118K & 2 & \textbf{49.33\%} & \textbf{64.30\%} & \textbf{18.80\%} & \textbf{8.58\%} & 34.07\% & 34.80\% & 24.57\% & 28.27\% \\
\bottomrule
\end{tabular}%
}
\end{table}

\subsection{Distillation Ablation: Gemma3 vs. o3 Model Comparison}

We also need to understand how the choice of language model inside the enhancement pipeline affects annotation quality and operating cost. Using o3 for every expression yields high-quality rewrites but is prohibitively expensive, while the off-the-shelf Gemma3-12B often hallucinates objects and locations that are absent from the imagery. To reconcile quality and cost, we distill o3 outputs~\cite{o3} into the Gemma3-Aerial model introduced in Chapter~\ref{chap:implement}~\cite{gemma3}, keeping the same prompting and decoding strategy across all generators. Fine-tuning uses QLoRA~\cite{qlora}, and large-scale inference runs efficiently with vLLM~\cite{vllm}.

Figure~\ref{fig:distillation_comparison} shows the qualitative effect: the base Gemma3 description invents a second baseball diamond, whereas the distilled model matches the grounded detail of the o3 response. Table~\ref{tab:cost_comparison} quantifies the economics. Processing roughly 300{,}000 targets with o3 would cost \$6{,}218.32, while the distilled Gemma3 executes locally for \$26.01—about 238× cheaper—because inference no longer depends on a commercial API. The saving is large enough to make multi-pass enhancements feasible even on academic budgets.

These findings confirm that distillation transfers both linguistic fidelity and grounding accuracy while drastically lowering the marginal cost of annotation. The distilled model therefore underpins all large-scale expression generation throughout the project.

\begin{figure*}[t]
\centering
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=0.65\textwidth]{Images/3llm.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\centering
\hspace{-1cm}
\raisebox{-0.3\height}{%
\footnotesize
\begin{tabular}{@{}p{2cm}p{5cm}@{}}
\toprule
\textbf{Expression Type} & \textbf{Example} \\
\midrule
Original & the orange baseball diamond in the top left \\
\midrule
o3 Enhanced & the orange baseball diamond with the light pole near home plate in the upper left \\
\midrule
Gemma3 Base & the bright orange baseball diamond to the left of another similar baseball diamond in the top left \\
\midrule
Gemma3-Aerial-12B & the orange baseball field with a chainlink fence surrounded by grass to the north and trees to the west \\
\bottomrule
\end{tabular}%
}
\end{minipage}
\caption{Qualitative comparison between o3, the base Gemma3 model, and the fine-tuned Gemma3-Aerial-12B model on aerial imagery. The sequence shows how each model enhances the original rule-based expression using the same prompt and decoding setup.}
\label{fig:distillation_comparison}
\end{figure*}

\begin{table}[t]
\centering
\caption{Cost analysis: Gemma3 vs. o3 model for large-scale annotation\protect\footnotemark}
\label{tab:cost_comparison}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model} & \textbf{Cost per request} & \textbf{Cost for 300K requests} \\
\midrule
o3 Model & \$0.020728 & \$6{,}218.32 \\
Distilled Gemma3 & \$0.000087 & \$26.01 \\
\midrule
\textbf{Savings} & \textbf{238× cheaper} & \textbf{\$6{,}192.31 (99.6\%)} \\
\bottomrule
\end{tabular}%
}
\end{table}
\footnotetext{Cost calculations based on API pricing—o3: \$2.00 per million input tokens, \$8.00 per million output tokens (OpenAI API platform); Gemma3-12B: \$0.035 per million input tokens, \$0.141 per million output tokens (OpenRouter inference provider). Average tokens per request: o3 (1,670.8 input, 2,173.3 output), Gemma3 (1,330.0 input, 284.7 output). Calculations based on 15 sample requests.}

\subsection{Historic Filter Ablation Study}

Finally, we evaluate how much the historic-filter augmentation contributes to robustness by retraining the combined model without injecting filtered images. Models trained solely on clean, contemporary imagery often falter when archival photographs introduce monochrome toning, contrast loss, or sepia casts. Removing the filters exposes whether robustness stems from the augmentation or from dataset diversity alone.

Table~\ref{tab:historic_ablation_results} reports the outcome using the same metric format as the combined evaluation. Without filter injections the model gains a few points on the clean validation splits—for instance RRSIS-D climbs to \textbf{64.36\%} mIoU—but it loses resilience on the historic versions. RefSegRS suffers the most, dropping from \textcolor{blue}{32.79\%} to \textcolor{blue}{27.73\%} mIoU and from \textcolor{blue}{36.74\%} to \textcolor{blue}{32.21\%} oIoU when confronted with filtered validation imagery. The Aerial-D and NWPU-Refer historic splits also shed several points, illustrating that the augmentation specifically protects datasets whose semantics depend on subtle colour and contrast cues.

These results confirm that modest exposure to historic degradations during training is enough to stabilise performance when evaluation imagery undergoes the same transformations. We therefore keep the filter injections in the final combined recipe despite the small trade-off in clean-split accuracy.

\begin{table}[H]
\centering
\caption{Historic filter ablation study—combined model trained on all datasets without historic-filter augmentation.}
\label{tab:historic_ablation_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{IoU@0.5} & \textbf{IoU@0.7} & \textbf{IoU@0.9} & \multicolumn{2}{c}{\textbf{mIoU}} & \multicolumn{2}{c}{\textbf{oIoU}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & & & \textbf{Orig.} & \textbf{Hist.} & \textbf{Orig.} & \textbf{Hist.} \\
\midrule
Aerial-D & 59.48\% (\textcolor{red}{-0.62}) & 43.37\% (\textcolor{red}{-0.58}) & 12.60\% (\textcolor{red}{-0.45}) & 49.21\% (\textcolor{red}{-0.57}) & \textcolor{blue}{43.57\%} (\textcolor{red}{-3.25}) & 62.88\% (\textcolor{red}{-0.56}) & \textcolor{blue}{58.14\%} (\textcolor{red}{-2.98}) \\
RRSIS-D & 74.60\% (\textcolor{green!60!black}{+5.12}) & 58.39\% (\textcolor{green!60!black}{+4.77}) & 20.75\% (\textcolor{red}{-1.03}) & 64.36\% (\textcolor{green!60!black}{+2.66}) & \textcolor{blue}{59.02\%} (\textcolor{green!60!black}{+0.67}) & 75.59\% (\textcolor{green!60!black}{+2.15}) & \textcolor{blue}{72.50\%} (\textcolor{green!60!black}{+0.77}) \\
NWPU-Refer & 46.02\% (\textcolor{green!60!black}{+5.49}) & 32.32\% (\textcolor{green!60!black}{+4.67}) & 10.66\% (\textcolor{green!60!black}{+2.21}) & 41.06\% (\textcolor{green!60!black}{+3.17}) & \textcolor{blue}{33.42\%} (\textcolor{green!60!black}{+1.90}) & 58.35\% (\textcolor{green!60!black}{+7.01}) & \textcolor{blue}{53.04\%} (\textcolor{green!60!black}{+6.87}) \\
RefSegRS & 47.33\% (\textcolor{green!60!black}{+6.03}) & 13.23\% (\textcolor{green!60!black}{+4.18}) & 1.16\% (\textcolor{red}{-0.93}) & 41.31\% (\textcolor{green!60!black}{+1.21}) & \textcolor{blue}{27.73\%} (\textcolor{red}{-5.06}) & 50.30\% (\textcolor{green!60!black}{+5.25}) & \textcolor{blue}{32.21\%} (\textcolor{red}{-4.53}) \\
Urban1960SatSeg & 78.46\% (\textcolor{green!60!black}{+0.41}) & 60.98\% (\textcolor{green!60!black}{+1.02}) & 28.66\% (\textcolor{green!60!black}{+0.41}) & 69.81\% (\textcolor{green!60!black}{+0.46}) & N/A & 87.80\% (\textcolor{green!60!black}{+0.53}) & N/A \\
\bottomrule
\end{tabular}%
}
\end{table}

Collectively, the updated experiments demonstrate that the Aerial-D dataset, RSRefSeg backbone, and LLM-enhanced expressions form a cohesive system that generalises across aerial benchmarks, withstands historic degradations, and remains economically scalable. These results ground the dissertation's claims with the most recent evidence and align the thesis with the accompanying article.
