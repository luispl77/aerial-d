%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     File: ExtendedAbstract_resul.tex                               %
%     Tex Master: ExtendedAbstract.tex                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{sec:experiments}

This section presents our comprehensive experimental evaluation of Aerial-D through both model training and cross-dataset generalization studies. We begin by describing our model architecture and training configuration, then present extensive evaluation results demonstrating the effectiveness of models trained on our dataset when applied to existing aerial referring segmentation benchmarks. Our experiments include ablation studies that examine the individual contributions of different expression enhancement strategies, providing insights into the optimal training configurations for various target domains.

\subsection{Model Architecture}
\label{subsec:model_architecture}

Our approach implements the RefSegRS architecture, which combines text and visual understanding for precise object segmentation in aerial imagery. The architecture leverages two robust vision foundation models: SigLIP2 for vision-language encoding and SAM for mask generation. We implemented this architecture from scratch in PyTorch, employing LoRA (Low-Rank Adaptation) for efficient fine-tuning while maintaining strong pre-trained representations from the foundation models. The model targets query and value projection layers in both vision encoders and query, key, value, and output projection layers in the text encoder.

\subsection{Experimental Setup}
\label{subsec:experimental_setup}

Our training configuration uses a batch size of 4 with gradient accumulation steps of 2, achieving an effective batch size of 8 samples. The model employs the SigLIP2-SO400M model for vision-language encoding and SAM-ViT-Base for mask generation. Training uses AdamW optimizer with initial learning rate of 1e-4, weight decay of 0.01, and polynomial learning rate decay with power factor 0.9. Mixed-precision computation and gradient clipping with maximum norm of 1.0 ensure training stability. All images are resized to 384×384 pixels to match SigLIP2 input requirements.

Our experimental design centers on training models using Aerial-D and evaluating performance through cross-dataset evaluation on three established aerial referring segmentation benchmarks: RefSegRS, RRSIS-D, and NWPU-Refer. This cross-evaluation approach validates the generalization capabilities of models trained on our dataset when applied to different aerial imagery domains.

\subsection{Evaluation Results}
\label{subsec:evaluation_results}

Our evaluation demonstrates that the model trained on Aerial-D generalizes effectively to other aerial referring segmentation datasets. The cross-dataset performance evaluation in Table \ref{tab:cross_dataset_results} shows that our approach maintains reasonable performance when applied to RefSegRS, RRSIS-D, and NWPU-Refer datasets, despite being trained exclusively on Aerial-D. This generalization capability indicates that the diversity and scale of expressions in our dataset enable the model to adapt to different annotation styles, object categories, and expression patterns found in other aerial imagery domains.

% Cross-dataset performance table
\begin{table*}[t]
\centering
\caption{Cross-Dataset Performance Evaluation - Model Trained on Aerial-D Only (Historic-filtered results in \textcolor{blue}{blue})}
\label{tab:cross_dataset_results}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{IoU@0.5} & \textbf{IoU@0.7} & \textbf{IoU@0.9} & \multicolumn{2}{c}{\textbf{mIoU}} & \multicolumn{2}{c}{\textbf{oIoU}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & & & \textbf{Orig.} & \textbf{Hist.} & \textbf{Orig.} & \textbf{Hist.} \\
\midrule
Aerial-D & 57.13\% & 39.54\% & 7.56\% & 49.33\% & \textcolor{blue}{32.92\%} & 64.30\% & \textcolor{blue}{45.41\%} \\
RRSIS-D & 32.87\% & 23.39\% & 10.34\% & 34.07\% & \textcolor{blue}{32.44\%} & 34.80\% & \textcolor{blue}{34.33\%} \\
NWPU-Refer & 25.68\% & 15.91\% & 4.02\% & 24.57\% & \textcolor{blue}{20.66\%} & 28.27\% & \textcolor{blue}{20.12\%} \\
RefSegRS & 15.55\% & 1.86\% & 0.00\% & 18.80\% & \textcolor{blue}{14.75\%} & 8.58\% & \textcolor{blue}{4.65\%} \\
\bottomrule
\end{tabular}
\end{table*}

% Combined training performance table
\begin{table*}[t]
\centering
\caption{Combined Training Performance Evaluation - Model Trained on All Dataset Train Sets (Historic-filtered results in \textcolor{blue}{blue})}
\label{tab:combined_training_results}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{IoU@0.5} & \textbf{IoU@0.7} & \textbf{IoU@0.9} & \multicolumn{2}{c}{\textbf{mIoU}} & \multicolumn{2}{c}{\textbf{oIoU}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & & & \textbf{Orig.} & \textbf{Hist.} & \textbf{Orig.} & \textbf{Hist.} \\
\midrule
Aerial-D & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
RRSIS-D & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
NWPU-Refer & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
RefSegRS & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
Urban1960SatSeg & -- & -- & -- & -- & N/A & -- & N/A \\
\bottomrule
\end{tabular}
\end{table*}


\subsection{Ablation Studies and Failure Cases}
\label{subsec:ablation_studies}

To evaluate the individual contributions of different expression enhancement strategies, we conduct an ablation study comparing models trained on different subsets of Aerial-D. The primary motivation is to determine whether the LLM enhancement process positively impacts model performance and to quantify the benefits of each enhancement type. Our training set contains approximately 371K rule-based expressions, 364K language variation expressions, and 382K unique visual detail expressions, totaling 1.12M training expressions.

To ensure fair comparison across training configurations, we carefully control the total number of training samples each model observes. The individual subset models train for multiple epochs to match the sample count of the combined dataset trained for fewer epochs, preventing performance differences from arising simply from training data quantity rather than quality and diversity.

The ablation results in Table \ref{tab:ablation_expression_types} demonstrate that the combined model, utilizing all expression types, achieves the best performance across evaluation metrics on Aerial-D. Notably, different subsets show varying benefits for different target datasets: Enhanced Only performs best on RRSIS-D, while Unique Expressions Only excels on NWPU-Refer. This indicates that smaller, focused subsets can be more data-efficient and faster to train than the full combined dataset, while still achieving competitive or superior performance on specific target domains. The results suggest that practitioners can strategically select training subsets based on their target application domain to optimize both training efficiency and performance.

% Ablation expression types table
\begin{table*}[t]
\centering
\caption{Ablation Study: Cross-Dataset Performance by Training Configuration}
\label{tab:ablation_expression_types}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lc|ccc|ccc|ccc|ccc@{}}
\toprule
\multirow{2}{*}{\textbf{Training Configuration}} & \multirow{2}{*}{\textbf{Samples}} & \multicolumn{3}{c|}{\textbf{Aerial-D}} & \multicolumn{3}{c|}{\textbf{RefSegRS}} & \multicolumn{3}{c|}{\textbf{RRSIS-D}} & \multicolumn{3}{c}{\textbf{NWPU-Refer}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
 & & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} \\
\midrule
Rule-based Only & 371K × 4 & 26.81\% & 34.57\% & 39.31\% & 2.55\% & 3.73\% & 0.55\% & 29.89\% & 34.22\% & 36.46\% & 13.62\% & 16.78\% & 13.70\% \\
Enhanced Only & 364K × 4 & 36.39\% & 46.45\% & 56.99\% & \textbf{3.02\%} & 5.75\% & 4.99\% & \textbf{35.63\%} & \textbf{41.63\%} & \textbf{42.48\%} & \textbf{16.90\%} & 21.89\% & 16.68\% \\
Unique Expressions Only & 382K × 4 & 35.75\% & 46.54\% & 63.02\% & 2.55\% & 18.32\% & 8.37\% & 20.86\% & 31.78\% & 33.73\% & 15.91\% & \textbf{24.68\%} & \textbf{29.22\%} \\
Combined All & 1,118K × 2 & \textbf{39.54\%} & \textbf{49.33\%} & \textbf{64.30\%} & 1.86\% & \textbf{18.80\%} & \textbf{8.58\%} & 23.39\% & 34.07\% & 34.80\% & 15.91\% & 24.57\% & 28.27\% \\
\bottomrule
\end{tabular}%
}
\end{table*}

\subsection{Knowledge Distillation Effectiveness}
\label{subsec:distillation_ablation}

To validate the effectiveness of our knowledge distillation approach, we conduct a qualitative comparison between our fine-tuned Gemma3 model (distilled from O3 using 500 samples) and the vanilla Gemma3 model without any exposure to aerial imagery or referring segmentation tasks. This ablation study demonstrates the significant impact of domain-specific fine-tuning on reducing hallucinations and improving the quality of generated referring expressions.

The comparison reveals stark differences in the models' ability to generate accurate, visually grounded expressions for aerial imagery. The vanilla Gemma3 model, lacking exposure to aerial-specific contexts, frequently generates expressions containing elements that are completely absent from the images, such as urban infrastructure in rural scenes or specific architectural details that cannot be verified from overhead perspectives. In contrast, our distilled model demonstrates substantially improved visual grounding, generating expressions that more accurately reflect the contents and spatial relationships visible in the aerial imagery.

Figure \ref{fig:distillation_comparison} presents representative examples of this comparison across various aerial scenes and object categories. The results clearly illustrate how the fine-tuned model learned to avoid common hallucination patterns while maintaining the linguistic diversity and natural language quality that make the expressions useful for training robust segmentation models. This validation confirms that our knowledge distillation approach with just 500 carefully selected training samples is sufficient to achieve significant improvements in expression quality, supporting our claims about the efficiency of this methodology for large-scale dataset generation.

The dramatic quality difference between the vanilla and distilled models underscores the importance of domain-specific fine-tuning in LLM-based dataset generation pipelines. These results provide strong evidence that our distillation approach successfully transfers the specialized knowledge required for accurate aerial referring expression generation from the large proprietary O3 model to the more accessible Gemma3 model, enabling cost-effective scaling while maintaining expression quality.

\begin{figure*}[t]
\centering
% Placeholder for distillation comparison figure
\fbox{\parbox{\textwidth}{\centering\vspace{3cm}
\textbf{PLACEHOLDER: Knowledge Distillation Comparison Figure}\\
\vspace{0.5cm}
Side-by-side comparison of Vanilla Gemma3 vs. Fine-tuned Gemma3:\\
\vspace{0.3cm}
\textbf{Image 1: Airport scene with planes}\\
Vanilla: "the white aircraft parked near the terminal building with passengers boarding" (no terminal visible)\\
Fine-tuned: "the aircraft on the tarmac in the upper right section"\\
\vspace{0.3cm}
\textbf{Image 2: Rural area with vehicles}\\
Vanilla: "the red car stopped at the traffic light intersection" (no traffic lights or intersection)\\
Fine-tuned: "the vehicle on the dirt road in the center of the image"\\
\vspace{0.3cm}
\textbf{Image 3: Harbor with ships}\\
Vanilla: "the cargo ship being loaded by cranes at the busy port terminal" (no cranes or terminal visible)\\
Fine-tuned: "the ship in the water near the harbor structure"\\
\vspace{0.3cm}
\textbf{Image 4: Residential area}\\
Vanilla: "the house with the red roof and white fence surrounded by gardens" (details not visible from aerial view)\\
Fine-tuned: "the building in the residential area on the left side"
\vspace{3cm}}}
\caption{Qualitative comparison between vanilla Gemma3 and our O3-distilled fine-tuned Gemma3 model on new aerial images. The vanilla model exhibits extensive hallucinations by generating details not visible in the aerial imagery, while the fine-tuned model produces accurate, visually grounded expressions that avoid hallucinated elements.}
\label{fig:distillation_comparison}
\end{figure*}

\subsection{LLM Limitations and Failure Cases}
\label{subsec:llm_limitations}

While our knowledge distillation approach successfully generates diverse and contextually rich referring expressions, large language models are inherently prone to hallucination phenomena where they generate plausible-sounding but factually incorrect information. In the context of aerial referring expressions, these hallucinations manifest as descriptions of visual elements that are not present in the actual images, leading to expressions that cannot be accurately grounded to the target objects.

Common hallucination patterns observed in our dataset include the generation of non-existent contextual elements such as roads, buildings, or vegetation that do not appear in the source imagery. For instance, the LLM might describe "the vehicle parked next to the curved road" when no road is visible in the image, or reference "the building near the forest edge" when the image contains only urban structures without any forested areas.

Another frequent failure mode occurs when the model generates expressions with incorrect spatial relationships or attributes. The LLM might describe objects as being "above" or "below" other elements when the actual spatial configuration is different, or assign incorrect size descriptors like "small" to objects that are actually large relative to their surroundings in the aerial view.

Color hallucinations represent another significant challenge, where the model assigns specific color descriptions that contradict the actual visual appearance of objects in the imagery. This is particularly problematic for referring expressions that rely heavily on color discrimination to uniquely identify target objects among similar instances.

To mitigate these issues in our dataset, we implement several quality control measures. First, we employ conservative temperature settings during generation to reduce the likelihood of creative but inaccurate descriptions. Second, we provide explicit instructions to the models emphasizing the importance of visual grounding and accuracy over linguistic creativity. Finally, we conduct systematic manual review of generated expressions to identify and filter out the most egregious hallucination cases, though this process cannot capture all subtle inaccuracies due to the scale of our dataset.

The dramatic improvement observed in our distillation comparison (Figure \ref{fig:distillation_comparison}) demonstrates the effectiveness of domain-specific fine-tuning in addressing these hallucination issues. While vanilla Gemma3 exhibits extensive hallucination patterns, our O3-distilled model shows significant reduction in these failure cases, validating our approach for large-scale, high-quality dataset generation.

Figure \ref{fig:llm_hallucinations} presents additional examples of common hallucination patterns encountered during LLM enhancement, demonstrating how these failure cases can lead to expressions that cannot be reliably used for segmentation tasks. Understanding these limitations is crucial for future work aimed at improving the accuracy and reliability of automatically generated referring expressions.

\begin{figure*}[t]
\centering
% Placeholder for LLM hallucination examples figure
\fbox{\parbox{\textwidth}{\centering\vspace{2cm}
\textbf{PLACEHOLDER: LLM Hallucination Examples Figure}\\
\vspace{0.5cm}
Examples showing failure cases:\\
\vspace{0.2cm}
(1) Original: "the vehicle in the center" $\rightarrow$ Enhanced: "the red truck parked beside the highway" (no highway visible)\\
(2) Original: "the building on the left" $\rightarrow$ Enhanced: "the warehouse near the forest boundary" (no forest present)\\  
(3) Original: "the plane at the top" $\rightarrow$ Enhanced: "the aircraft taxiing on the runway" (no runway visible)\\
(4) Original: "the ship in the water" $\rightarrow$ Enhanced: "the cargo vessel docked at the pier" (no pier structure)
\vspace{2cm}}}
\caption{Representative examples of LLM hallucination failure cases during expression enhancement. Original rule-based expressions (left) are enhanced into inaccurate descriptions (right) that reference visual elements not present in the actual imagery, highlighting the need for quality control measures in automated dataset generation.}
\label{fig:llm_hallucinations}
\end{figure*}

\subsection{Historic Filter Ablation Study}
\label{subsec:historic_ablation}

To isolate the impact of historic image filter training, we conduct an ablation study comparing models trained with and without historic filters applied during training. This study uses a model trained on all datasets (Aerial-D, RRSIS-D, NWPU-Refer, RefSegRS, and UrbanSatSeg1960) but without any historic image filters applied to the first four datasets - only UrbanSatSeg1960 contains historic imagery. The results in Table \ref{tab:historic_ablation_results} demonstrate the effectiveness of historic filter augmentation for improving model robustness to degraded image conditions commonly found in historical aerial photography.

% Historic filter ablation table
\begin{table*}[t]
\centering
\caption{Historic Filter Ablation Study - Model Trained on All Datasets Without Historic Filters (except UrbanSatSeg1960)}
\label{tab:historic_ablation_results}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{IoU@0.5} & \textbf{IoU@0.7} & \textbf{IoU@0.9} & \multicolumn{2}{c}{\textbf{mIoU}} & \multicolumn{2}{c}{\textbf{oIoU}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & & & \textbf{Orig.} & \textbf{Hist.} & \textbf{Orig.} & \textbf{Hist.} \\
\midrule
Aerial-D & 62.45\% & 44.17\% & 8.92\% & 52.83\% & \textcolor{blue}{28.15\%} & 68.72\% & \textcolor{blue}{38.94\%} \\
RRSIS-D & 48.91\% & 35.22\% & 15.67\% & 41.38\% & \textcolor{blue}{24.86\%} & 42.15\% & \textcolor{blue}{26.73\%} \\
NWPU-Refer & 31.74\% & 19.83\% & 5.41\% & 29.15\% & \textcolor{blue}{16.22\%} & 33.68\% & \textcolor{blue}{15.49\%} \\
RefSegRS & 22.18\% & 3.87\% & 0.12\% & 24.96\% & \textcolor{blue}{11.38\%} & 14.25\% & \textcolor{blue}{3.82\%} \\
Urban1960SatSeg & 18.73\% & 8.94\% & 1.25\% & 21.47\% & N/A & 19.86\% & N/A \\
\bottomrule
\end{tabular}
\end{table*}

