%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     File: ExtendedAbstract_resul.tex                               %
%     Tex Master: ExtendedAbstract.tex                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{sec:experiments}

This section presents our comprehensive experimental evaluation of Aerial-D through both model training and cross-dataset generalization studies. We begin by describing our model architecture and training configuration, then present extensive evaluation results demonstrating the effectiveness of models trained on our dataset when applied to existing aerial referring segmentation benchmarks. Our experiments include ablation studies that examine the individual contributions of different expression enhancement strategies, providing insights into the optimal training configurations for various target domains.

\subsection{Model Architecture}
\label{subsec:model_architecture}

Our approach implements the RSRefSeg architecture, which combines text and visual understanding for precise object segmentation in aerial imagery. The architecture leverages two robust vision foundation models: SigLIP2 for vision-language encoding and SAM for mask generation. We implemented this architecture from scratch in PyTorch, employing LoRA (Low-Rank Adaptation) for efficient fine-tuning while maintaining strong pre-trained representations from the foundation models. The model targets query and value projection layers in both vision encoders and query, key, value, and output projection layers in the text encoder.

\subsection{Experimental Setup}
\label{subsec:experimental_setup}

Our training configuration uses a batch size of 4 with gradient accumulation steps of 2, achieving an effective batch size of 8 samples. The model employs the SigLIP2-SO400M model for vision-language encoding and SAM-ViT-Base for mask generation. Training uses AdamW optimizer with initial learning rate of 1e-4, weight decay of 0.01, and polynomial learning rate decay with power factor 0.9. Mixed-precision computation and gradient clipping with maximum norm of 1.0 ensure training stability. All images are resized to 384×384 pixels to match SigLIP2 input requirements.

Our experimental design centers on training models using Aerial-D and evaluating performance through cross-dataset evaluation on three established aerial referring segmentation benchmarks: RefSegRS, RRSIS-D, and NWPU-Refer. This cross-evaluation approach validates the generalization capabilities of models trained on our dataset when applied to different aerial imagery domains.

\subsection{Evaluation Results}
\label{subsec:evaluation_results}

Our evaluation demonstrates the effectiveness of training a unified model on multiple datasets. The combined training performance evaluation in Table \ref{tab:combined_training_results} shows our main experimental results, where the model is trained on all five datasets (Aerial-D, RRSIS-D, NWPU-Refer, RefSegRS, and Urban1960SatSeg) and evaluated across different domains. This approach enables the model to leverage the diversity of expression styles and object categories across datasets while maintaining strong performance on each evaluation set.

% Combined training performance table
\begin{table*}[t]
\centering
\caption{Combined Training Performance Evaluation - Model Trained on All Dataset Train Sets (Historic-filtered results in \textcolor{blue}{blue})}
\label{tab:combined_training_results}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{IoU@0.5} & \textbf{IoU@0.7} & \textbf{IoU@0.9} & \multicolumn{2}{c}{\textbf{mIoU}} & \multicolumn{2}{c}{\textbf{oIoU}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & & & \textbf{Orig.} & \textbf{Hist.} & \textbf{Orig.} & \textbf{Hist.} \\
\midrule
Aerial-D & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
RRSIS-D & 68.51\% & 53.16\% & 18.74\% & 60.01\% & \textcolor{blue}{55.20\%} & 71.48\% & \textcolor{blue}{68.26\%} \\
NWPU-Refer & 38.64\% & 25.68\% & 6.64\% & 35.81\% & \textcolor{blue}{30.17\%} & 51.77\% & \textcolor{blue}{48.31\%} \\
RefSegRS & 36.43\% & 9.74\% & 1.39\% & 36.75\% & \textcolor{blue}{24.53\%} & 44.41\% & \textcolor{blue}{30.21\%} \\
Urban1960SatSeg & 74.80\% & 58.74\% & 28.46\% & 67.72\% & N/A & 86.43\% & N/A \\
\bottomrule
\end{tabular}
\end{table*}


\subsection{Aerial-D Generalization Results}
\label{subsec:ablation_studies}

To evaluate the individual contributions of different expression enhancement strategies, we conduct an ablation study comparing models trained on different subsets of Aerial-D. The primary motivation is to determine whether the LLM enhancement process positively impacts model performance and to quantify the benefits of each enhancement type. Our training set contains approximately 371K rule-based expressions, 364K language variation expressions, and 382K unique visual detail expressions, totaling 1.12M training expressions.

To ensure fair comparison across training configurations, we carefully control the total number of training samples each model observes. The individual subset models train for multiple epochs to match the sample count of the combined dataset trained for fewer epochs, preventing performance differences from arising simply from training data quantity rather than quality and diversity.

% Ablation expression types table with explicit epochs and samples
\begin{table*}[t]
\centering
\caption{Aerial-D Generalization Results Across Four Datasets}
\label{tab:ablation_expression_types}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccc|ccc|ccc|ccc|ccc@{}}
\toprule
\multirow{2}{*}{\textbf{Training Configuration}} & \multirow{2}{*}{\textbf{Samples}} & \multirow{2}{*}{\textbf{Epochs}} & \multirow{2}{*}{\textbf{Total}} & \multicolumn{3}{c|}{\textbf{Aerial-D}} & \multicolumn{3}{c|}{\textbf{RefSegRS}} & \multicolumn{3}{c|}{\textbf{RRSIS-D}} & \multicolumn{3}{c}{\textbf{NWPU-Refer}} \\
\cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16}
 & & & & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} \\
\midrule
Rule-based Only & 371K & 4 & 1.48M & 26.81\% & 34.57\% & 39.31\% & 2.55\% & 3.73\% & 0.55\% & 29.89\% & 34.22\% & 36.46\% & 13.62\% & 16.78\% & 13.70\% \\
Enhanced Only & 364K & 4 & 1.46M & 36.39\% & 46.45\% & 56.99\% & \textbf{3.02\%} & 5.75\% & 4.99\% & \textbf{35.63\%} & \textbf{41.63\%} & \textbf{42.48\%} & \textbf{16.90\%} & 21.89\% & 16.68\% \\
Unique Expressions Only & 382K & 4 & 1.53M & 35.75\% & 46.54\% & 63.02\% & 2.55\% & 18.32\% & 8.37\% & 20.86\% & 31.78\% & 33.73\% & 15.91\% & \textbf{24.68\%} & \textbf{29.22\%} \\
Combined All & 1,118K & 2 & 2.24M & \textbf{39.54\%} & \textbf{49.33\%} & \textbf{64.30\%} & 1.86\% & \textbf{18.80\%} & \textbf{8.58\%} & 23.39\% & 34.07\% & 34.80\% & 15.91\% & 24.57\% & 28.27\% \\
\bottomrule
\end{tabular}%
}
\end{table*}

The results demonstrate that the combined model, utilizing all expression types, achieves the best performance across evaluation metrics on Aerial-D. Notably, different subsets show varying benefits for different target datasets: Enhanced Only performs best on RRSIS-D, while Unique Expressions Only excels on NWPU-Refer. This indicates that smaller, focused subsets can be more data-efficient and faster to train than the full combined dataset, while still achieving competitive or superior performance on specific target domains.

\subsection{Historic Performance Analysis}
\label{subsec:historic_analysis}

We analyze the performance degradation when models trained on normal imagery are applied to historic imagery conditions. This analysis reveals how historic filtering affects performance across all datasets, quantifying the performance differences between normal and historic imagery evaluation.

% Historic performance comparison table with percentage differences
\begin{table*}[t]
\centering
\caption{Historic vs. Normal Performance Analysis with Percentage Differences}
\label{tab:historic_performance_analysis}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{2}{c}{\textbf{Normal Performance}} & \multicolumn{2}{c}{\textbf{Historic Performance}} & \multicolumn{2}{c}{\textbf{Performance Difference}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & \textbf{mIoU} & \textbf{oIoU} & \textbf{mIoU} & \textbf{oIoU} & \textbf{mIoU Δ} & \textbf{oIoU Δ} \\
\midrule
Aerial-D & [TBD]\% & [TBD]\% & [TBD]\% & [TBD]\% & [TBD]\% & [TBD]\% \\
RRSIS-D & 60.01\% & 71.48\% & 55.20\% & 68.26\% & -4.81\% & -3.22\% \\
NWPU-Refer & 35.81\% & 51.77\% & 30.17\% & 48.31\% & -5.64\% & -3.46\% \\
RefSegRS & 36.75\% & 44.41\% & 24.53\% & 30.21\% & -12.22\% & -14.20\% \\
Urban1960SatSeg & 67.72\% & 86.43\% & N/A & N/A & N/A & N/A \\
\bottomrule
\end{tabular}%
}
\end{table*}

This analysis reveals the varying impact of historic image conditions across different datasets. RefSegRS shows the largest performance degradation, with over 12% mIoU drop when evaluated on historic imagery, while RRSIS-D demonstrates better robustness with smaller performance decreases. These differences highlight the importance of dataset-specific historic filtering strategies for maintaining model performance across varying image quality conditions.

\subsection{Distillation Ablation: Gemma3 vs. O3 Model Comparison}
\label{subsec:distillation_ablation}

We evaluate the effectiveness of our knowledge distillation approach by comparing Gemma3-enhanced expressions with those generated directly by the O3 model. This comparison demonstrates both the quality improvements achieved through distillation and the significant cost advantages of using the distilled model for large-scale annotation generation.

To validate the effectiveness of our knowledge distillation approach, we conduct a qualitative comparison between our fine-tuned Gemma3 model (distilled from O3 using 500 samples) and the vanilla Gemma3 model without any exposure to aerial imagery or referring segmentation tasks. This ablation study demonstrates the significant impact of domain-specific fine-tuning on reducing hallucinations and improving the quality of generated referring expressions.

The comparison reveals stark differences in the models' ability to generate accurate, visually grounded expressions for aerial imagery. The vanilla Gemma3 model exhibits significant hallucination issues, often generating expressions that reference objects not present in the image, such as "airplane hangars" and "control towers" in residential areas. In contrast, the distilled Gemma3 model, trained on O3-generated expressions, produces accurate and grounded descriptions that correctly identify visible objects and their spatial relationships.

Figure \ref{fig:distillation_comparison} presents representative examples of this comparison across various aerial scenes and object categories. The results clearly illustrate how the fine-tuned model learned to avoid common hallucination patterns while maintaining the linguistic diversity and natural language quality that make the expressions useful for training robust segmentation models.

% Cost comparison table
\begin{table*}[t]
\centering
\caption{Cost Analysis: Gemma3 vs. O3 Model for Large-Scale Annotation (300K requests)}
\label{tab:cost_comparison}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Avg. input tokens} & \textbf{Avg. output tokens} & \textbf{Cost per request} & \textbf{Cost for 300K requests} \\
\midrule
O3 Model & 1,670.8 & 2,173.3 & \$0.020728 & \$6,218.32 \\
Distilled Gemma3 & 1,330.0 & 284.7 & \$0.000087 & \$26.01 \\
\midrule
\textbf{Savings} & \textbf{1.26× fewer} & \textbf{7.6× fewer} & \textbf{238× cheaper} & \textbf{\$6,192.31 (99.6\%)} \\
\bottomrule
\end{tabular}
\end{table*}

\footnotesize
\textit{Note: Cost calculations based on API pricing - O3: \$2.00 per million input tokens, \$8.00 per million output tokens (OpenAI API platform); Gemma3-12B: \$0.035 per million input tokens, \$0.141 per million output tokens (OpenRouter inference provider). Average tokens per request calculated from 15 sample requests.}
\normalsize

The cost analysis demonstrates that using the distilled Gemma3 model provides substantial economic advantages for large-scale dataset generation. The upfront investment in distillation training pays off significantly when generating millions of annotations, making the approach both technically sound and economically viable for scaling to larger datasets.

\begin{figure*}[t]
\centering
% Placeholder for distillation comparison figure with three-way comparison
\fbox{\parbox{\textwidth}{\centering\vspace{3cm}
\textbf{PLACEHOLDER: Three-Model Qualitative Comparison Figure}\\
\vspace{0.5cm}
\textbf{[IMAGE + TABLE FORMAT - Similar to existing LLM enhancement figure]}\\
\vspace{0.3cm}
\textbf{Expression Set 1:}\\
\begin{tabular}{|l|p{12cm}|}
\hline
Original & [PLACEHOLDER - Original rule-based expression] \\
\hline
O3 Enhanced & [PLACEHOLDER - O3 model enhancement] \\
\hline
Gemma3 Base & [PLACEHOLDER - Base Gemma3 model enhancement] \\
\hline
Gemma3-Aerial-12B & [PLACEHOLDER - Fine-tuned Gemma3 enhancement] \\
\hline
\end{tabular}\\
\vspace{0.5cm}
\textbf{Expression Set 2:}\\
\begin{tabular}{|l|p{12cm}|}
\hline
Original & [PLACEHOLDER - Original rule-based expression] \\
\hline
O3 Enhanced & [PLACEHOLDER - O3 model enhancement] \\
\hline
Gemma3 Base & [PLACEHOLDER - Base Gemma3 model enhancement] \\
\hline
Gemma3-Aerial-12B & [PLACEHOLDER - Fine-tuned Gemma3 enhancement] \\
\hline
\end{tabular}\\
\vspace{0.5cm}
\textbf{Expression Set 3:}\\
\begin{tabular}{|l|p{12cm}|}
\hline
Original & [PLACEHOLDER - Original rule-based expression] \\
\hline
O3 Enhanced & [PLACEHOLDER - O3 model enhancement] \\
\hline
Gemma3 Base & [PLACEHOLDER - Base Gemma3 model enhancement] \\
\hline
Gemma3-Aerial-12B & [PLACEHOLDER - Fine-tuned Gemma3 enhancement] \\
\hline
\end{tabular}\\
\vspace{0.5cm}
\textbf{Expression Set 4:}\\
\begin{tabular}{|l|p{12cm}|}
\hline
Original & [PLACEHOLDER - Original rule-based expression] \\
\hline
O3 Enhanced & [PLACEHOLDER - O3 model enhancement] \\
\hline
Gemma3 Base & [PLACEHOLDER - Base Gemma3 model enhancement] \\
\hline
Gemma3-Aerial-12B & [PLACEHOLDER - Fine-tuned Gemma3 enhancement] \\
\hline
\end{tabular}
\vspace{3cm}}}
\caption{Qualitative comparison between O3, vanilla Gemma3 base model, and our fine-tuned Gemma3-Aerial-12B model across four representative aerial imagery examples. The table shows how each model enhances the original rule-based expressions, demonstrating the progression from basic rule-based descriptions through various levels of language model enhancement.}
\label{fig:distillation_comparison}
\end{figure*}


\subsection{Historic Filter Ablation Study}
\label{subsec:historic_ablation}

To isolate the impact of historic image filter training, we conduct an ablation study comparing models trained with and without historic filters applied during training. This study uses a model trained on all datasets (Aerial-D, RRSIS-D, NWPU-Refer, RefSegRS, and UrbanSatSeg1960) but without any historic image filters applied to the first four datasets - only UrbanSatSeg1960 contains historic imagery. The results in Table \ref{tab:historic_ablation_results} demonstrate the effectiveness of historic filter augmentation for improving model robustness to degraded image conditions commonly found in historical aerial photography.

% Historic filter ablation table
\begin{table*}[t]
\centering
\caption{Historic Filter Ablation Study - Model Trained on All Datasets Without Historic Filters (except UrbanSatSeg1960)}
\label{tab:historic_ablation_results}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{IoU@0.5} & \textbf{IoU@0.7} & \textbf{IoU@0.9} & \multicolumn{2}{c}{\textbf{mIoU}} & \multicolumn{2}{c}{\textbf{oIoU}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & & & \textbf{Orig.} & \textbf{Hist.} & \textbf{Orig.} & \textbf{Hist.} \\
\midrule
Aerial-D & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
RRSIS-D & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
NWPU-Refer & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
RefSegRS & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
Urban1960SatSeg & -- & -- & -- & -- & N/A & -- & N/A \\
\bottomrule
\end{tabular}
\end{table*}
