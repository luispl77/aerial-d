%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     File: ExtendedAbstract_resul.tex                               %
%     Tex Master: ExtendedAbstract.tex                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{sec:experiments}

\subsection{Model Architecture}
\label{subsec:model_architecture}

Our approach implements the RefSegRS architecture, which combines text and visual understanding for precise object segmentation in aerial imagery. The architecture leverages two robust vision foundation models: SigLIP2 for vision-language encoding and SAM for mask generation. We implemented this architecture from scratch in PyTorch, employing LoRA (Low-Rank Adaptation) for efficient fine-tuning while maintaining strong pre-trained representations from the foundation models. The model targets query and value projection layers in both vision encoders and query, key, value, and output projection layers in the text encoder.

\subsection{Experimental Setup}
\label{subsec:experimental_setup}

Our training configuration uses a batch size of 4 with gradient accumulation steps of 2, achieving an effective batch size of 8 samples. The model employs the SigLIP2-SO400M model for vision-language encoding and SAM-ViT-Base for mask generation. Training uses AdamW optimizer with initial learning rate of 1e-4, weight decay of 0.01, and polynomial learning rate decay with power factor 0.9. Mixed-precision computation and gradient clipping with maximum norm of 1.0 ensure training stability. All images are resized to 384×384 pixels to match SigLIP2 input requirements.

Our experimental design centers on training models using Aerial-D and evaluating performance through cross-dataset evaluation on three established aerial referring segmentation benchmarks: RefSegRS, RRSIS-D, and NWPU-Refer. This cross-evaluation approach validates the generalization capabilities of models trained on our dataset when applied to different aerial imagery domains.

\subsection{Evaluation Results}
\label{subsec:evaluation_results}

Our evaluation demonstrates that the model trained on Aerial-D generalizes effectively to other aerial referring segmentation datasets. The cross-dataset performance evaluation in Table \ref{tab:cross_dataset_results} shows that our approach maintains reasonable performance when applied to RefSegRS, RRSIS-D, and NWPU-Refer datasets, despite being trained exclusively on Aerial-D. This generalization capability indicates that the diversity and scale of expressions in our dataset enable the model to adapt to different annotation styles, object categories, and expression patterns found in other aerial imagery domains.

% Cross-dataset performance table
\begin{table*}[t]
\centering
\caption{Cross-Dataset Performance Evaluation - Model Trained on Aerial-D Only (Historic-filtered results in \textcolor{blue}{blue})}
\label{tab:cross_dataset_results}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{IoU@0.5} & \textbf{IoU@0.7} & \textbf{IoU@0.9} & \multicolumn{2}{c}{\textbf{mIoU}} & \multicolumn{2}{c}{\textbf{oIoU}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & & & \textbf{Orig.} & \textbf{Hist.} & \textbf{Orig.} & \textbf{Hist.} \\
\midrule
Aerial-D & 57.13\% & 39.54\% & 7.56\% & 49.33\% & \textcolor{blue}{32.92\%} & 64.30\% & \textcolor{blue}{45.41\%} \\
RRSIS-D & 32.87\% & 23.39\% & 10.34\% & 34.07\% & \textcolor{blue}{32.44\%} & 34.80\% & \textcolor{blue}{34.33\%} \\
NWPU-Refer & 25.68\% & 15.91\% & 4.02\% & 24.57\% & \textcolor{blue}{20.66\%} & 28.27\% & \textcolor{blue}{20.12\%} \\
RefSegRS & 15.55\% & 1.86\% & 0.00\% & 18.80\% & \textcolor{blue}{14.75\%} & 8.58\% & \textcolor{blue}{4.65\%} \\
\bottomrule
\end{tabular}
\end{table*}

% Combined training performance table
\begin{table*}[t]
\centering
\caption{Combined Training Performance Evaluation - Model Trained on All Dataset Train Sets (Historic-filtered results in \textcolor{blue}{blue})}
\label{tab:combined_training_results}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{IoU@0.5} & \textbf{IoU@0.7} & \textbf{IoU@0.9} & \multicolumn{2}{c}{\textbf{mIoU}} & \multicolumn{2}{c}{\textbf{oIoU}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & & & \textbf{Orig.} & \textbf{Hist.} & \textbf{Orig.} & \textbf{Hist.} \\
\midrule
Aerial-D & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
RRSIS-D & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
NWPU-Refer & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
RefSegRS & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
Urban1960SatSeg & -- & -- & -- & -- & N/A & -- & N/A \\
\bottomrule
\end{tabular}
\end{table*}


\subsection{Ablation Studies}
\label{subsec:ablation_studies}

To evaluate the individual contributions of different expression enhancement strategies, we conduct an ablation study comparing models trained on different subsets of Aerial-D. The primary motivation is to determine whether the LLM enhancement process positively impacts model performance and to quantify the benefits of each enhancement type. Our training set contains approximately 371K rule-based expressions, 364K language variation expressions, and 382K unique visual detail expressions, totaling 1.12M training expressions.

To ensure fair comparison across training configurations, we carefully control the total number of training samples each model observes. The individual subset models train for multiple epochs to match the sample count of the combined dataset trained for fewer epochs, preventing performance differences from arising simply from training data quantity rather than quality and diversity.

The ablation results in Table \ref{tab:ablation_expression_types} demonstrate that the combined model, utilizing all expression types, achieves the best performance across evaluation metrics on Aerial-D. Notably, different subsets show varying benefits for different target datasets: Enhanced Only performs best on RRSIS-D, while Unique Expressions Only excels on NWPU-Refer. This indicates that smaller, focused subsets can be more data-efficient and faster to train than the full combined dataset, while still achieving competitive or superior performance on specific target domains. The results suggest that practitioners can strategically select training subsets based on their target application domain to optimize both training efficiency and performance.

% Ablation expression types table
\begin{table*}[t]
\centering
\caption{Ablation Study: Cross-Dataset Performance by Training Configuration}
\label{tab:ablation_expression_types}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lc|ccc|ccc|ccc|ccc@{}}
\toprule
\multirow{2}{*}{\textbf{Training Configuration}} & \multirow{2}{*}{\textbf{Samples}} & \multicolumn{3}{c|}{\textbf{Aerial-D}} & \multicolumn{3}{c|}{\textbf{RefSegRS}} & \multicolumn{3}{c|}{\textbf{RRSIS-D}} & \multicolumn{3}{c}{\textbf{NWPU-Refer}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
 & & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} & \textbf{Pass@0.7} & \textbf{mIoU} & \textbf{oIoU} \\
\midrule
Rule-based Only & 371K × 4 & 26.81\% & 34.57\% & 39.31\% & 2.55\% & 3.73\% & 0.55\% & 29.89\% & 34.22\% & 36.46\% & 13.62\% & 16.78\% & 13.70\% \\
Enhanced Only & 364K × 4 & 36.39\% & 46.45\% & 56.99\% & \textbf{3.02\%} & 5.75\% & 4.99\% & \textbf{35.63\%} & \textbf{41.63\%} & \textbf{42.48\%} & \textbf{16.90\%} & 21.89\% & 16.68\% \\
Unique Expressions Only & 382K × 4 & 35.75\% & 46.54\% & 63.02\% & 2.55\% & 18.32\% & 8.37\% & 20.86\% & 31.78\% & 33.73\% & 15.91\% & \textbf{24.68\%} & \textbf{29.22\%} \\
Combined All & 1,118K × 2 & \textbf{39.54\%} & \textbf{49.33\%} & \textbf{64.30\%} & 1.86\% & \textbf{18.80\%} & \textbf{8.58\%} & 23.39\% & 34.07\% & 34.80\% & 15.91\% & 24.57\% & 28.27\% \\
\bottomrule
\end{tabular}%
}
\end{table*}

