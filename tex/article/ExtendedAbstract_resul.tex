%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     File: ExtendedAbstract_resul.tex                               %
%     Tex Master: ExtendedAbstract.tex                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{sec:experiments}

This section presents a comprehensive experimental evaluation of Aerial-D that spans model training, cross-dataset generalization, and targeted ablations. We first describe the model architecture and training setup, then report cross-dataset results on established aerial referring segmentation benchmarks. Beyond aggregate performance, we also include: (i) ablation of expression enhancement strategies; (ii) ablation of historic-filter training; and (iii) qualitative comparison across language models (o3, base Gemma3, and our distilled Gemma3-Aerial model), coupled with a cost analysis of these alternatives.

\subsection{Model Architecture}
\label{subsec:model_architecture}

In selecting the architecture for our experiments, we prioritized a model that can interpret natural-language descriptions while producing pixel-accurate masks in aerial scenes. Generic segmentation systems rarely meet both requirements under the domain shift of remote sensing, where objects are small, repetitive, and densely arranged. We therefore adopt RSRefSeg as our backbone for referring segmentation: it pairs a strong language–image encoder (SigLIP2) with a state-of-the-art mask decoder (SAM), enabling robust semantic grounding alongside precise mask generation. As in the original RSRefSeg training, the model is fine‑tuned via LoRA adapters; we retain this recipe when adapting to the remote‑sensing domain. This choice is further supported by the strong performance reported for RSRefSeg on RRSIS-D\cite{liu2024rotated}, providing a proven baseline for aerial referring segmentation and reducing risk in our evaluation.

Concretely, we implement the RSRefSeg architecture\cite{chen2025rsrefseg} in PyTorch, as illustrated in Figure~\ref{fig:rsrefseg_arch}. The design couples SigLIP2\cite{siglip2} for vision–language encoding with SAM\cite{sam} for mask generation. Following the original authors, training uses LoRA (Low‑Rank Adaptation)\cite{lora} adapters inserted into the SAM image encoder and into SigLIP2’s vision and text encoders, while preserving the strong pretraining of these backbones. In practice, adapters are placed on the query and value projection layers in the vision encoders and on the query, key, value, and output projection layers in the text encoder.

\subsection{Experimental Setup}
\label{subsec:experimental_setup}

Our training configuration uses a batch size of 4 with gradient accumulation steps of 2, achieving an effective batch size of 8 samples. The model employs \texttt{SigLIP2-SO400M} for vision–language encoding and \texttt{SAM-ViT-Large} for mask generation when training the combined model. Consistent with the RSRefSeg training strategy, we employ LoRA with rank \(r=32\) for parameter‑efficient adaptation. Training uses AdamW optimizer\cite{adamw} with initial learning rate of 1e-4, weight decay of 0.01, and polynomial learning rate decay with power factor 0.9. Mixed-precision computation and gradient clipping with maximum norm of 1.0 ensure training stability. All images are resized to 384×384 pixels to match SigLIP2 input requirements. Historic filters are applied to 20\% of training images; for each selected image, one of three variants—grayscale, grain/contrast roll-off, or sepia/noise—is chosen uniformly at random (probability 1/3 each).

We train a single combined model on all five datasets—Aerial-D, RRSIS-D\cite{liu2024rotated}, NWPU-Refer\cite{yang2024large}, RefSegRS\cite{yuan2023rrsis}, and Urban1960SatSeg\cite{hao2025urban1960satseg}—and evaluate on these same five datasets. To assess robustness under degradation beyond our own data, we also report results on historic-filtered variants of the three non-historic external datasets (RRSIS-D, NWPU-Refer, RefSegRS). In addition, to study generalization to a lighter decoder, we evaluate a configuration using \texttt{SAM-ViT-Base} at test time.

\subsection{Evaluation Results}
\label{subsec:evaluation_results}

Table~\ref{tab:combined_training_results} reports validation results for a single combined model trained jointly on all datasets and evaluated on each dataset's validation split. We present Pass@0.5/0.7/0.9 (shown as IoU@0.5/0.7/0.9 in the table), which measure the percentage of instances whose predicted mask attains IoU greater than or equal to the threshold. We also report two aggregate overlap scores: mIoU, the mean of per-instance IoU values, and oIoU, the overall IoU obtained by summing intersections and unions across all instances before taking their ratio.

For the four datasets that are not inherently historical, we additionally evaluate a "Hist." variant by applying the same historic filters used in training to the validation images. As expected, the historic variants exhibit a modest drop relative to the original images due to the added noise and grain introduced by these filters, while the combined model maintains competitive performance across domains.

Quantitatively, the combined model attains strong results across benchmarks. On RRSIS-D, it reaches Pass@0.5/0.7/0.9 of 68.51\%/53.16\%/18.74\% with mIoU 60.01\% and oIoU 71.48\%. On NWPU-Refer, it achieves 38.64\%/25.68\%/6.64\% with mIoU 35.81\% and oIoU 51.77\%. On RefSegRS, it reports 36.43\%/9.74\%/1.39\% with mIoU 36.75\% and oIoU 44.41\%. On Urban1960SatSeg, it reaches 74.80\%/58.74\%/28.46\% with mIoU 67.72\% and oIoU 86.43\%. Despite the diversity of expression styles and object categories across datasets, these scores match or surpass the author‑reported baselines for the four public benchmarks. For Aerial‑D, we further establish baseline performance in terms of mIoU and oIoU, providing a reference point for future work on this dataset.

% Combined training performance table
\begin{table*}[t]
\centering
\caption{Combined Training Performance Evaluation - Model Trained on All Dataset Train Sets (Historic-filtered results in \textcolor{blue}{blue})}
\label{tab:combined_training_results}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{IoU@0.5} & \textbf{IoU@0.7} & \textbf{IoU@0.9} & \multicolumn{2}{c}{\textbf{mIoU}} & \multicolumn{2}{c}{\textbf{oIoU}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & & & \textbf{Orig.} & \textbf{Hist.} & \textbf{Orig.} & \textbf{Hist.} \\
\midrule
Aerial-D & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
RRSIS-D & 68.51\% & 53.16\% & 18.74\% & 60.01\% & \textcolor{blue}{55.20\%} & 71.48\% & \textcolor{blue}{68.26\%} \\
NWPU-Refer & 38.64\% & 25.68\% & 6.64\% & 35.81\% & \textcolor{blue}{30.17\%} & 51.77\% & \textcolor{blue}{48.31\%} \\
RefSegRS & 36.43\% & 9.74\% & 1.39\% & 36.75\% & \textcolor{blue}{24.53\%} & 44.41\% & \textcolor{blue}{30.21\%} \\
Urban1960SatSeg & 74.80\% & 58.74\% & 28.46\% & 67.72\% & N/A & 86.43\% & N/A \\
\bottomrule
\end{tabular}
\end{table*}


\subsection{Expression Enhancement Ablation}
\label{subsec:ablation_studies}

We train four separate models on Aerial-D, each using a distinct subset of expressions to isolate the effect of different kinds of linguistic variation: (i) \emph{Rule-based Only} uses the original deterministic expressions generated by our rule-based system; (ii) \emph{Enhanced Only} uses LLM rephrasings that vary the wording while preserving the target; (iii) \emph{Unique Expressions Only} uses LLM augmentations that introduce alternative visual attributes and cues to reference the object; and (iv) \emph{Combined All} is the union of all three. Figure~\ref{fig:distillation_comparison} provides qualitative examples of LLM-enhanced phrasing relative to rule-based expressions.

We then evaluate these four models on four validation sets without additional training: Aerial-D (using the \emph{combined-all} validation split) and three external datasets—RefSegRS, RRSIS-D, and NWPU-Refer—on which the models were not trained in this ablation. This design tests how each expression type supports cross-dataset generalization.

Table~\ref{tab:ablation_expression_types} summarizes the results and explicitly reports both the number of \emph{Samples} and \emph{Epochs} used per configuration. Looking across datasets, several consistent patterns emerge. On Aerial-D, the \emph{Combined All} configuration achieves the best accuracy, which is expected given that the validation distribution matches that training mixture. For the three external datasets, different subsets yield the strongest generalization: on RRSIS-D, varying the language (\emph{Enhanced Only}) helps most; on NWPU-Refer, emphasizing varied visual cues (\emph{Unique Expressions Only}) is most beneficial; and on RefSegRS, a combination of both types provides the best results. These outcomes highlight the breadth of ways to phrase referring expressions and show that leveraging LLMs to introduce targeted variety improves cross-dataset generalization.

% Ablation expression types table with explicit epochs and samples
\begin{table*}[t]
\centering
\caption{Expression Enhancement Ablation Across Four Datasets}
\label{tab:ablation_expression_types}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcc|cc|cc|cc|cc@{}}
\toprule
\multirow{2}{*}{\textbf{Training Configuration}} & \multirow{2}{*}{\textbf{Samples}} & \multirow{2}{*}{\textbf{Epochs}} & \multicolumn{2}{c|}{\textbf{Aerial-D}} & \multicolumn{2}{c|}{\textbf{RefSegRS}} & \multicolumn{2}{c|}{\textbf{RRSIS-D}} & \multicolumn{2}{c}{\textbf{NWPU-Refer}} \\
\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
 & & & % \textbf{Pass@0.7} & 
\textbf{mIoU} & \textbf{oIoU} & % \textbf{Pass@0.7} & 
\textbf{mIoU} & \textbf{oIoU} & % \textbf{Pass@0.7} & 
\textbf{mIoU} & \textbf{oIoU} & % \textbf{Pass@0.7} & 
\textbf{mIoU} & \textbf{oIoU} \\
\midrule
Rule-based Only & 371K & 4 & % 26.81\% & 
34.57\% & 39.31\% & % 2.55\% & 
3.73\% & 0.55\% & % 29.89\% & 
34.22\% & 36.46\% & % 13.62\% & 
16.78\% & 13.70\% \\
Enhanced Only & 364K & 4 & % 36.39\% & 
46.45\% & 56.99\% & % \textbf{3.02\%} & 
5.75\% & 4.99\% & % \textbf{35.63\%} & 
\textbf{41.63\%} & \textbf{42.48\%} & % \textbf{16.90\%} & 
21.89\% & 16.68\% \\
Unique Expressions Only & 382K & 4 & % 35.75\% & 
46.54\% & 63.02\% & % 2.55\% & 
18.32\% & 8.37\% & % 20.86\% & 
31.78\% & 33.73\% & % 15.91\% & 
\textbf{24.68\%} & \textbf{29.22\%} \\
Combined All & 1,118K & 2 & % \textbf{39.54\%} & 
\textbf{49.33\%} & \textbf{64.30\%} & % 1.86\% & 
\textbf{18.80\%} & \textbf{8.58\%} & % 23.39\% & 
34.07\% & 34.80\% & % 15.91\% & 
24.57\% & 28.27\% \\
\bottomrule
\end{tabular}%
}
\end{table*}

Two additional observations follow from Table~\ref{tab:ablation_expression_types}. First, the epoch schedule reflects early stopping cues: validation loss begins to rise after the fourth epoch for the three smaller subsets and after the second epoch for the combined set, so we cap training at 4 and 2 epochs, respectively. Second, despite operating over fewer unique expressions, the subset configurations trained for four epochs often match or surpass the two-epoch combined baseline on external datasets. This suggests that LLM-produced variants are sample efficient—enabling more economical training or validation runs on Aerial-D while retaining or improving generalization to other datasets.


\subsection{Distillation Ablation: Gemma3 vs. o3 Model Comparison}
\label{subsec:distillation_ablation}

Generating high‑quality referring expressions at dataset scale demands a balance between quality and cost. Proprietary models such as o3 produce strong descriptions out of the box, but applying them to hundreds of thousands of targets is prohibitively expensive. Local open‑weights models like Gemma3 are affordable to run, yet the unadapted base model often hallucinates or misses aerial‑specific cues.

To resolve this, we first ask o3 to produce a small set of teaching examples: 500 targets, each with multiple expressions. We then train a local Gemma3‑12B to imitate these examples with supervised fine‑tuning. Once trained, this student model enhances the remainder of the dataset (\(\sim\)300K targets) efficiently on a single GPU.

This teacher–student training is knowledge distillation with o3 as the teacher and Gemma3‑12B as the student (Figure~\ref{fig:llm_distillation}). We implement fine‑tuning with QLoRA\cite{qlora}, and run large‑scale inference using vLLM\cite{vllm} on one NVIDIA RTX 6000; the full enhancement pass took approximately four days.

For evaluation, we compare three sources of expressions under an identical prompt and decoding setup: (i) o3 directly; (ii) the Gemma3 base model; and (iii) our distilled Gemma3‑Aerial model. Removing the distillation step (i.e., using the base model) leads to frequent hallucinations—references to objects not present—whereas the distilled student yields grounded descriptions aligned with visible structures and simple spatial relations.

The practical benefit appears in the cost analysis (Table~\ref{tab:cost_comparison}): replacing o3 with the distilled student reduces cost by roughly 238× for \(\sim\)300K requests, saving thousands of dollars while preserving the qualitative advantages of stronger supervision.

Figure \ref{fig:distillation_comparison} presents representative examples of this comparison across various aerial scenes and object categories. The results clearly illustrate how the fine-tuned model learned to avoid common hallucination patterns while maintaining the linguistic diversity and natural language quality that make the expressions useful for training robust segmentation models.

% Cost comparison table
\begin{table}[t]
\centering
\caption{Cost Analysis: Gemma3 vs. o3 Model for Large-Scale Annotation\protect\footnotemark}
\label{tab:cost_comparison}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model} & \textbf{Cost per request} & \textbf{Cost for 300K requests} \\
\midrule
o3 Model & \$0.020728 & \$6,218.32 \\
Distilled Gemma3 & \$0.000087 & \$26.01 \\
\midrule
\textbf{Savings} & \textbf{238× cheaper} & \textbf{\$6,192.31 (99.6\%)} \\
\bottomrule
\end{tabular}%
}
\end{table}
\footnotetext{Cost calculations based on API pricing — o3: \$2.00 per million input tokens, \$8.00 per million output tokens (OpenAI API platform); Gemma3-12B: \$0.035 per million input tokens, \$0.141 per million output tokens (OpenRouter inference provider). Average tokens per request: o3 (1,670.8 input, 2,173.3 output), Gemma3 (1,330.0 input, 284.7 output). Calculations based on 15 sample requests.}

The cost analysis demonstrates that using the distilled Gemma3 model provides substantial economic advantages for large-scale dataset generation. The upfront investment in distillation training pays off significantly when generating millions of annotations, making the approach both technically sound and economically viable for scaling to larger datasets.

\begin{figure*}[t]
\centering
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=0.65\textwidth]{./images/3llm.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\centering
\hspace{-1cm}
\raisebox{-0.3\height}{%
\footnotesize
\begin{tabular}{@{}p{2cm}p{5cm}@{}}
\toprule
\textbf{Expression Type} & \textbf{Example} \\
\midrule
Original & the orange baseball diamond in the top left \\
\midrule
o3 Enhanced & the orange baseball diamond with the light pole near home plate in the upper left \\
\midrule
Gemma3 Base & the bright orange baseball diamond to the left of another similar baseball diamond in the top left \\
\midrule
Gemma3-Aerial-12B & the orange baseball field with a chainlink fence surrounded by grass to the north and trees to the west \\
\bottomrule
\end{tabular}%
}
\end{minipage}
\caption{Qualitative comparison between o3, vanilla Gemma3 base model, and our fine-tuned Gemma3-Aerial-12B model on aerial imagery. The table shows how each model enhances the original rule-based expression using an identical prompt and decoding setup, demonstrating the progression from basic rule-based descriptions through increasingly capable LLM enhancements.}
\label{fig:distillation_comparison}
\end{figure*}


\subsection{Historic Filter Ablation Study}
\label{subsec:historic_ablation}

In order to verify that the model can handle degraded conditions typical of historical aerial imagery, we examine the contribution of training-time historic filters. Many archival scenes appear in grayscale, exhibit film grain, or suffer contrast roll-off; models trained only on modern imagery often lose accuracy under these conditions. We therefore design a controlled ablation to measure how much robustness these filters provide.

Table~\ref{tab:historic_ablation_results} reports this ablation. We replicate the main combined-training setup—same architecture, hyperparameters, and five-dataset mix (Aerial-D, RefSegRS, RRSIS-D, NWPU-Refer, Urban1960SatSeg)—but train a second combined model with historic filters disabled. The only difference from the main model is that we do not include the dataset-level historic-filter augmentation (applied prior to training to 20\% of images, selecting one of three filters with equal probability).

We evaluate this no-filter model on each dataset’s original validation split and on historic-filtered variants for the three non-historic external datasets. Relative to the main model (which includes pre-generated historic-filtered samples for 20\% of images—grayscale, grain/contrast roll-off, or sepia/noise chosen uniformly), the no-filter model shows a consistent, slight drop in the historical evaluation columns. This indicates a loss of robustness to degraded imagery and confirms that including these pre-generated historic-filtered samples helps preserve performance under historical conditions while maintaining accuracy on original imagery.

% Historic filter ablation table
\begin{table*}[t]
\centering
\caption{Historic Filter Ablation Study — Combined model trained on all datasets without historic-filter augmentation}
\label{tab:historic_ablation_results}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{IoU@0.5} & \textbf{IoU@0.7} & \textbf{IoU@0.9} & \multicolumn{2}{c}{\textbf{mIoU}} & \multicolumn{2}{c}{\textbf{oIoU}} \\
\cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & & & \textbf{Orig.} & \textbf{Hist.} & \textbf{Orig.} & \textbf{Hist.} \\
\midrule
Aerial-D & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
RRSIS-D & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
NWPU-Refer & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
RefSegRS & -- & -- & -- & -- & \textcolor{blue}{--} & -- & \textcolor{blue}{--} \\
Urban1960SatSeg & -- & -- & -- & -- & N/A & -- & N/A \\
\bottomrule
\end{tabular}
\end{table*}
