%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     File: ExtendedAbstract_abstr.tex                               %
%     Tex Master: ExtendedAbstract.tex                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

Referring expression segmentation represents a fundamental challenge in computer vision that integrates natural language understanding with precise visual localization. Existing datasets for referring expression segmentation focus primarily on natural scene imagery, leaving significant limitations in aerial domain applications where objects exhibit unique spatial configurations and contextual relationships. To facilitate the development of this field, we introduce Aerial-D, the largest referring expression segmentation dataset for aerial imagery to date, comprising 37,288 image patches with over 1.5 million referring expressions covering 259,709 annotated targets across individual objects, groups, and semantic categories spanning 21 distinct classes from vehicles and infrastructure to land cover types. The dataset represents the first fully automatic construction pipeline in this field, using systematic rule-based generation followed by Large Language Model enhancement that significantly enriched both the linguistic variety and visual detail richness of the referring expressions. We demonstrate good generalization results when models trained on Aerial-D are evaluated on other aerial segmentation datasets, highlighting the dataset's effectiveness for aerial referring expression tasks. The dataset is publicly available at \url{https://huggingface.co/datasets/luisml77/aerial-d}.

\vspace{0.3cm}

\noindent{{\bf Keywords:}} Aerial imagery, referring expression segmentation, dataset, large language models, computer vision

\end{abstract}

