%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     File: ExtendedAbstract_abstr.tex                               %
%     Tex Master: ExtendedAbstract.tex                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization. Applying this to aerial imagery presents unique challenges due to high object densities and geographic complexities. We introduce Aerial-D, the largest referring expression segmentation dataset for aerial imagery to date, comprising 37,288 image patches with over 1.5 million referring expressions covering 259,709 annotated targets across individual objects, groups, and semantic categories spanning 21 distinct classes from vehicles and infrastructure to land cover types. The dataset represents the first fully automatic construction pipeline in this field, using systematic rule-based expression generation followed by Large Language Model enhancement that significantly enriched both the linguistic variety and visual detail richness of the referring expressions. We train one model on Aerial-D together with prior aerial benchmarks, yielding unified instance, semantic, and historic segmentation from text, performing strongly on every dataset we evaluate

\vspace{0.3cm}

\noindent{{\bf Keywords:}} Aerial imagery, referring expression segmentation, dataset, large language models, computer vision

\end{abstract}
