%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     File: ExtendedAbstract_concl.tex                               %
%     Tex Master: ExtendedAbstract.tex                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion and Future Work}
\label{sec:conclusion}

This work presents a comprehensive approach to open-vocabulary segmentation of aerial photographs through the development of AerialSeg and the AerialD dataset. By building upon the RSRefSeg architecture and implementing a systematic pipeline for dataset generation, we have demonstrated the feasibility of extending referring segmentation capabilities to the aerial domain.

Our rule-based annotation system successfully transforms existing semantic and instance segmentation datasets into rich referring segmentation resources, enabling the creation of natural language descriptions for objects in overhead imagery. The integration of spatial relationships, size comparisons, and color analysis provides a foundation for generating diverse and contextually relevant referring expressions that capture the unique characteristics of aerial scenes.

The enhancement of our dataset through large language models represents a significant advancement in dataset quality and diversity. By leveraging models such as Gemma3, we have shown that automatically generated referring expressions can be refined to achieve more natural and varied language patterns while maintaining semantic accuracy. This approach demonstrates the potential for scalable dataset creation that combines rule-based precision with the linguistic sophistication of modern language models.

Our experimental results validate the effectiveness of the RSRefSeg architecture when adapted to aerial imagery, showing promising performance on the task of open-vocabulary segmentation in overhead scenes. The combination of SigLIP and SAM backbones, enhanced through parameter-efficient LoRA fine-tuning, provides a robust foundation for handling the unique challenges of aerial image understanding.

\subsection{Future Work}

The field of open-vocabulary aerial image segmentation presents numerous opportunities for advancement, particularly through the integration of emerging multimodal technologies. A promising direction lies in leveraging new large language models that possess pixel-level segmentation capabilities, such as vision-language models with inherent understanding of spatial relationships and object boundaries.

These advanced models could serve dual purposes in future work. First, they could replace or augment the current RSRefSeg architecture as the base segmentation model, potentially offering more sophisticated understanding of the relationship between natural language descriptions and visual content. Their integrated approach to vision and language processing could eliminate the need for separate encoder architectures and complex fusion mechanisms.

Second, and perhaps more significantly, such models could revolutionize the dataset generation process itself. Beyond their current use for enhancing referring expressions, pixel-aware language models could generate entirely new segmentation masks based on textual descriptions, creating truly synthetic yet realistic aerial imagery datasets. This capability would enable the creation of datasets that are not constrained by the original annotations of existing semantic segmentation datasets, allowing for the generation of novel object configurations, rare scenarios, and specialized use cases that may be difficult to capture in real-world aerial photography.

This approach would make the segmentation system truly general in what it can segment, moving beyond the limitations of predefined object categories toward genuine open-vocabulary capabilities. The combination of advanced language models for both architecture and data generation could establish a new paradigm for aerial image understanding that is both more flexible and more comprehensive than current approaches.

Another significant opportunity lies in expanding Aerial-D from its current English-only format to a comprehensive multilingual resource. The knowledge distillation approach demonstrated in this work provides an ideal framework for multilingual dataset expansion, as large proprietary models like OpenAI's O3 possess sophisticated understanding of over 100 languages and can effectively capture the subtle linguistic nuances inherent in referring expressions across different languages. Our distillation methodology could leverage these capabilities to teach student models like Gemma3 the specialized patterns needed for translating referring remote sensing expressions while preserving their semantic accuracy and spatial specificity. This multilingual expansion would democratize access to aerial referring segmentation capabilities across diverse linguistic communities and enable cross-cultural applications in remote sensing analysis.

A particularly promising direction involves leveraging emerging large language models capable of generating segmentation masks to enable semi-automatic dataset creation. Our results demonstrate that knowledge distillation with just 500 carefully selected samples is sufficient to produce models with significant hallucination reduction, indicating that large language models require relatively small amounts of training data to learn specialized tasks effectively. This characteristic makes them exceptionally well-suited to serve as automatic data annotators. By fine-tuning pixel-aware LLMs on small, high-quality datasets using our distillation approach, we could create powerful automated annotation systems that combine the mask generation capabilities of these advanced models with the domain-specific accuracy achieved through targeted fine-tuning. This semi-automatic approach would dramatically reduce the manual effort required for dataset creation while maintaining the quality standards necessary for robust model training, potentially enabling the rapid development of specialized referring segmentation datasets for diverse aerial imaging applications.

Furthermore, the LLM enhancement component and distillation methodology demonstrated in this work can be directly applied to the other four datasets used in our training (RRSIS-D, NWPU-Refer, RefSegRS, and Urban1960SatSeg). By leveraging our established distillation pipeline with domain-specific fine-tuning, these existing datasets could be significantly expanded with high-quality, diverse referring expressions. This expansion would result in even larger volumes of enhanced training data, potentially improving model performance and generalization capabilities across different aerial imagery domains. The scalability of our approach makes such comprehensive enhancement feasible while maintaining cost-effectiveness through the use of distilled models rather than expensive proprietary language models.

