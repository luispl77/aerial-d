%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     File: ExtendedAbstract_concl.tex                               %
%     Tex Master: ExtendedAbstract.tex                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion and Future Work}
\label{sec:conclusion}

This work introduces Aerial\mbox{-}D and an end-to-end methodology for converting existing aerial segmentation resources into a large-scale referring segmentation dataset. We design a rule-based pipeline that turns masks into natural descriptions through spatial relations, relative size, and color cues, and we enhance linguistic variety via LLM-based rewriting and cost-efficient distillation. On the modeling side, we implement the RSRefSeg model architecture that couples SigLIP2 for language–image grounding with SAM for precise mask decoding, adapted to remote sensing using LoRA. Trained jointly across multiple aerial benchmarks, the combined model matches or surpasses author-reported baselines on public datasets while establishing clear mIoU/oIoU baselines on Aerial\mbox{-}D itself, including robustness analyses with historic image filters.

Future work could explore three concrete extensions. First, scaling the rule‑based and LLM enhancement pipeline across the public benchmarks used here could form a single, much larger corpus with greater intra‑class variety and phrasing diversity than Aerial\mbox{-}D alone, together with consistent normalization and de‑duplication across sources. Second, multilingual expansion via the same distillation recipe is attractive: large multilingual models (e.g., o3, Gemini) can draft high‑quality rewrites, and a compact student (e.g., Gemma3) can learn these patterns to produce accurate, scalable translations of entire expression sets while preserving spatial cues. Third, this work used language models to enrich referring expressions while keeping masks and targets fixed to the annotations and class taxonomies of the source datasets; in contrast, recent large multimodal models (e.g., Gemini 2.5\cite{gemini25}) can output pixel‑level segmentation masks out of the box. Such systems could propose new targets and generate fresh instance masks directly from prompts—enabling the creation of new instance segmentation datasets or augmentation of existing ones (e.g., iSAID, LoveDA)—after which the same referring‑expression pipeline could be applied to yield less restricted datasets with broader open‑vocabulary coverage.
