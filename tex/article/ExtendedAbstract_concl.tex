%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     File: ExtendedAbstract_concl.tex                               %
%     Tex Master: ExtendedAbstract.tex                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion and Future Work}
\label{sec:conclusion}

This work introduces Aerial-D together with an end-to-end methodology that converts existing aerial segmentation datasets into a large-scale repository of referring expressions. The pipeline begins with a rule-driven generator that translates masks into prose grounded on location, appearance, and relational cues, then refines that language through LLM rewriting while keeping costs manageable via a distilled Gemma3 annotator. The resulting corpus enables RSRefSeg to be trained jointly across five datasets, establishes reproducible baselines on Aerial-D, and remains competitive with published results on RRSIS-D, NWPU-Refer, RefSegRS, and Urban1960SatSeg. By pairing these evaluations with ablations on expression sources and historic-image filters, we demonstrate that Aerial-D delivers a harder dataset for referring segmentation in aerial photos and highlights the specific ingredients that improve robustness.

Future work can extend this foundation in three directions. First, the expression-enhancement pipeline can be applied directly to the native captions supplied with public datasets such as RRSIS-D and NWPU-Refer, enriching their language with the visual grounding cues that proved effective for Aerial-D and creating a unified, higher-variety training pool. Second, multilingual variants of these expressions can be produced while preserving full automation by pairing high-quality translation models with our distillation recipe—either prompting o3 and training a Gemma3 student to mimic those translations or seeding the process with dedicated systems such as Tower~\cite{tower}. Third, emerging multimodal systems like Gemini 2.5\cite{gemini25} already output full segmentation masks and bounding boxes; integrating them could expand Aerial-D with additional targets derived from the same imagery, which can then be described with our expression-generation stages to unlock richer open-vocabulary supervision.
