#!/usr/bin/env python3
"""
Fine-tune Gemma 3 for aerial imagery referring expression generation using synthetic dataset
Based on data generated by gemini_enhance.py
"""

import torch
import argparse
import os
import json
import glob
from datasets import Dataset
from PIL import Image
from transformers import AutoProcessor, AutoModelForImageTextToText
from trl import SFTConfig, SFTTrainer
from huggingface_hub import login

def parse_args():
    parser = argparse.ArgumentParser(description='Fine-tune Gemma 3 for aerial referring expressions')
    parser.add_argument('--model_id', type=str, default="google/gemma-3-4b-it",
                       help='Hugging Face model ID')
    parser.add_argument('--enhanced_dir', type=str, default="./enhanced_annotations",
                       help='Directory containing enhanced annotations')
    parser.add_argument('--output_dir', type=str, default="gemma-aerial-referring",
                       help='Output directory for fine-tuned model')
    parser.add_argument('--gpu', type=int, default=0, help='GPU ID to use')
    parser.add_argument('--epochs', type=int, default=2, help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=1, help='Per device batch size')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=4,
                       help='Gradient accumulation steps')
    parser.add_argument('--learning_rate', type=float, default=2e-5, help='Learning rate')
    parser.add_argument('--hf_token', type=str, help='Hugging Face token')
    return parser.parse_args()

def setup_device_and_login(args):
    """Setup device and login to Hugging Face"""
    device = f"cuda:{args.gpu}" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # Login to Hugging Face if token provided
    if args.hf_token:
        login(args.hf_token)
        print("Logged in to Hugging Face Hub")
    
    return device

def load_enhanced_dataset(enhanced_dir):
    """Load and prepare the enhanced aerial dataset"""
    print("Loading enhanced aerial dataset...")
    
    # System message that matches the gemini_enhance.py prompt
    system_message = (
        "You are an expert at creating natural language descriptions for objects in aerial imagery. "
        "Your task is to help create diverse and precise referring expressions for objects in the image. "
        "The target object is highlighted with a red bounding box\n\n"
        
        "You have three tasks:\n\n"
        
        "TASK 1: For each original expression listed below, create EXACTLY 1 language variation that:\n"
        "1. MUST PRESERVE ALL SPATIAL INFORMATION from the original expression:\n"
        "   - Absolute positions (e.g., \"in the top right\", \"near the center\")\n"
        "   - Relative positions (e.g., \"to the right of\", \"below\")\n"
        "2. Use natural, everyday language that a regular person would use\n"
        "   - Avoid overly formal or technical vocabulary\n"
        "   - Use common synonyms (e.g., \"car\" instead of \"automobile\")\n"
        "   - Keep the tone conversational and straightforward\n"
        "3. Ensure the variation uniquely identifies this object to avoid ambiguity\n\n"
        
        "TASK 2: Analyze the object's context and uniqueness factors:\n"
        "1. Examine the immediate surroundings of the object\n"
        "2. Identify distinctive features that could be used to uniquely identify this object:\n"
        "   - Nearby objects and their relationships\n"
        "   - Visual characteristics that distinguish it from similar objects\n"
        "   - Environmental context (roads, buildings, terrain) that provide reference points\n"
        "3. Consider how the original automated expressions could be improved\n"
        "4. Focus on features that would help someone locate this specific object without ambiguity\n\n"
        
        "TASK 3: Generate EXACTLY 2 new expressions that:\n"
        "1. MUST be based on one of the original expressions or their variations\n"
        "2. Add visual details ONLY when you are highly confident about them\n"
        "3. Each expression must uniquely identify this object\n"
        "4. Focus on describing the object's relationship with its immediate surroundings\n"
        "5. Maintain the core spatial information from the original expression\n\n"
        
        "You must return your output in the following JSON format:\n"
        "{\n"
        "  \"enhanced_expressions\": [\n"
        "    {\n"
        "      \"original_expression\": \"<original expression>\",\n"
        "      \"variation\": \"<single language variation>\"\n"
        "    },\n"
        "    ...\n"
        "  ],\n"
        "  \"unique_description\": \"<detailed analysis of spatial context and uniqueness factors>\",\n"
        "  \"unique_expressions\": [\n"
        "    \"<new expression based on original 1>\",\n"
        "    \"<new expression based on original 2>\"\n"
        "  ]\n"
        "}\n"
        "Only return the JSON object, no other text or comments.\n"
        "Write all the expressions using lowercase letters and no punctuation."
    )
    
    # Find all enhanced annotation files
    json_files = glob.glob(os.path.join(enhanced_dir, "*", "enhanced_expressions.json"))
    print(f"Found {len(json_files)} enhanced annotation files")
    
    formatted_dataset = []
    
    for json_file in json_files:
        try:
            # Load the JSON data
            with open(json_file, 'r') as f:
                data = json.load(f)
            
            # Find corresponding image file
            obj_dir = os.path.dirname(json_file)
            image_files = glob.glob(os.path.join(obj_dir, "*.png"))
            if not image_files:
                print(f"Warning: No image found for {json_file}")
                continue
            
            image_path = image_files[0]
            
            # Create user prompt that includes the original expressions
            formatted_expressions = "\n".join([f"- {expr}" for expr in data["original_expressions"]])
            user_prompt = (
                f"Create language variations of the provided expressions while preserving spatial information, "
                f"analyze the spatial context for uniqueness factors, and generate new unique expressions for this {data['category']} "
                "(highlighted with a red bounding box).\n\n"
                f"ORIGINAL EXPRESSIONS TO ENHANCE:\n{formatted_expressions}"
            )
            
            # Create the expected response JSON
            response_json = json.dumps(data["enhanced_data"], indent=2)
            
            # Convert to messages format
            sample = {
                "messages": [
                    {
                        "role": "system",
                        "content": [{"type": "text", "text": system_message}],
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "image": Image.open(image_path).convert("RGB"),
                            },
                            {
                                "type": "text",
                                "text": user_prompt,
                            },
                        ],
                    },
                    {
                        "role": "assistant",
                        "content": [{"type": "text", "text": response_json}],
                    },
                ],
            }
            
            formatted_dataset.append(sample)
            
        except Exception as e:
            print(f"Error processing {json_file}: {e}")
            continue
    
    print(f"Successfully loaded {len(formatted_dataset)} samples")
    print("Sample data structure:")
    if formatted_dataset:
        print("System message preview:")
        print(formatted_dataset[0]["messages"][0]["content"][0]["text"][:200] + "...")
        print("\nUser prompt preview:")
        print(formatted_dataset[0]["messages"][1]["content"][1]["text"][:200] + "...")
    
    return formatted_dataset, system_message

def process_vision_info(messages):
    """Process vision information from messages"""
    image_inputs = []
    # Iterate through each conversation
    for msg in messages:
        # Get content (ensure it's a list)
        content = msg.get("content", [])
        if not isinstance(content, list):
            content = [content]

        # Check each content element for images
        for element in content:
            if isinstance(element, dict) and (
                "image" in element or element.get("type") == "image"
            ):
                # Get the image and convert to RGB
                if "image" in element:
                    image = element["image"]
                else:
                    image = element
                image_inputs.append(image.convert("RGB"))
    return image_inputs

def create_collate_fn(processor):
    """Create collate function for training"""
    def collate_fn(examples):
        texts = []
        images = []
        for example in examples:
            image_inputs = process_vision_info(example["messages"])
            text = processor.apply_chat_template(
                example["messages"], add_generation_prompt=False, tokenize=False
            )
            texts.append(text.strip())
            images.append(image_inputs)

        # Tokenize the texts and process the images
        batch = processor(text=texts, images=images, return_tensors="pt", padding=True)

        # The labels are the input_ids, and we mask the padding tokens and image tokens in the loss computation
        labels = batch["input_ids"].clone()

        # Mask image tokens
        image_token_id = [
            processor.tokenizer.convert_tokens_to_ids(
                processor.tokenizer.special_tokens_map["boi_token"]
            )
        ]
        # Mask tokens for not being used in the loss computation
        labels[labels == processor.tokenizer.pad_token_id] = -100
        labels[labels == image_token_id] = -100
        labels[labels == 262144] = -100

        batch["labels"] = labels
        return batch
    
    return collate_fn

def setup_model_and_processor(args, device):
    """Setup model and processor"""
    print(f"Loading model: {args.model_id}")
    
    # Load model without quantization
    model = AutoModelForImageTextToText.from_pretrained(
        args.model_id,
        torch_dtype=torch.bfloat16,
        device_map=f"cuda:{args.gpu}" if torch.cuda.is_available() else "cpu",
        attn_implementation="eager"
    )
    processor = AutoProcessor.from_pretrained(args.model_id)
    
    print("Model and processor loaded successfully")
    return model, processor

def train_model(model, processor, dataset, args):
    """Train the model using SFTTrainer"""
    print("Starting training...")
    
    # Create training configuration
    training_args = SFTConfig(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        bf16=True,
        logging_steps=1,
        save_strategy="no",  # Don't save during training to avoid tied weights issue
        gradient_checkpointing=True,
        warmup_ratio=0.03,
        lr_scheduler_type="constant",
        report_to="tensorboard",
        remove_unused_columns=False,
        dataset_text_field="",  # need a dummy field for collator
        dataset_kwargs={"skip_prepare_dataset": True},  # important for collator
    )
    
    # Create collate function
    collate_fn = create_collate_fn(processor)
    
    # Create trainer
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        processing_class=processor,
        data_collator=collate_fn,
    )
    
    # Start training
    trainer.train()
    
    # Save the final model using the recommended method for tied weights
    #trainer.save_model()
    trainer.model.save_pretrained(args.output_dir, safe_serialization=False)
    processor.save_pretrained(args.output_dir)
    
    print("Training completed!")
    return trainer



def main():
    args = parse_args()
    
    # Setup device and login
    device = setup_device_and_login(args)
    
    # Create dataset
    dataset, system_message = load_enhanced_dataset(args.enhanced_dir)
    
    if not dataset:
        print("No training data found!")
        return
    
    # Setup model and processor
    model, processor = setup_model_and_processor(args, device)
    
    # Train model
    trainer = train_model(model, processor, dataset, args)
    
    # Free memory
    del model
    del trainer
    torch.cuda.empty_cache()
    
    print("Fine-tuning completed successfully!")

if __name__ == "__main__":
    main() 